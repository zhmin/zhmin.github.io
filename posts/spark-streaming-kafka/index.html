<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.88.1" /><meta name="theme-color" content="#fff" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>Spark Streaming Kafka 原理 | 学习笔记</title>

    <link rel="stylesheet" href="/css/meme.min.2d3e1834af2bc8dda20c7d74aaa1d8134845094441b7a554a5206ccd4b472596.css"/>

    
    
        <script src="/js/meme.min.800f61672145711eb3e1f29fcfcc3dbe0139dd459259718b611a0390f74703d1.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="zhmin" /><meta name="description" content="Spark Streaming Kafka 原理 Kafka作为一个消息队列，具有很高的吞吐量，和Spark Streaming结合起来，可以实现高速实时的流处理。Spark Streaming在以前的版本支持两种方式读取Kafka，一种是通过……" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="学习笔记" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="学习笔记" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://zhmin.github.io/posts/spark-streaming-kafka/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2019-02-21T21:25:04+00:00",
        "dateModified": "2021-10-22T14:44:34+08:00",
        "url": "https://zhmin.github.io/posts/spark-streaming-kafka/",
        "headline": "Spark Streaming Kafka 原理",
        "description": "Spark Streaming Kafka 原理 Kafka作为一个消息队列，具有很高的吞吐量，和Spark Streaming结合起来，可以实现高速实时的流处理。Spark Streaming在以前的版本支持两种方式读取Kafka，一种是通过……",
        "inLanguage" : "zh-CN",
        "articleSection": "posts",
        "wordCount":  3286 ,
        "image": ["https://zhmin.github.io/spark-streaming-kafka-direct.svg"],
        "author": {
            "@type": "Person",
            "description": "无问西东",
            "email": "myzhmin@gmail.com",
            "image": "https://zhmin.github.io/icons/apple-touch-icon.png",
            "url": "https://zhmin.github.io/",
            "name": "zhmin"
        },
        "publisher": {
            "@type": "Organization",
            "name": "学习笔记",
            "logo": {
                "@type": "ImageObject",
                "url": "https://zhmin.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://zhmin.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://zhmin.github.io/"
        }
    }
</script>

    
    



<meta property="og:title" content="Spark Streaming Kafka 原理" />
<meta property="og:description" content="Spark Streaming Kafka 原理 Kafka作为一个消息队列，具有很高的吞吐量，和Spark Streaming结合起来，可以实现高速实时的流处理。Spark Streaming在以前的版本支持两种方式读取Kafka，一种是通过……" />
<meta property="og:url" content="https://zhmin.github.io/posts/spark-streaming-kafka/" />
<meta property="og:site_name" content="学习笔记" />
<meta property="og:locale" content="zh" /><meta property="og:image" content="https://zhmin.github.io/spark-streaming-kafka-direct.svg" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2019-02-21T21:25:04&#43;00:00" />
    <meta property="article:modified_time" content="2021-10-22T14:44:34&#43;08:00" />
    
    <meta property="article:section" content="posts" />



    
    

    
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">学习笔记</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7 0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6 0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6 0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3 0 64v48c0 8.8 7.2 16 16 16h480c8.8 0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class="menu-item-name">文章</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255 0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255 0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255 0-24 10.745-24 24zm386.667-56H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24z"/></svg><span class="menu-item-name">分类</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">标签</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">关于</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">Spark Streaming Kafka 原理</h1>

            

            
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2019-02-21T21:25:04&#43;00:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;2019.2.21</time>
    
    
    
    
        
        
        
            
                <span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href="/categories/spark-streaming/" class="category-link p-category">spark streaming</a></span>
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;3286</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;7&nbsp;分钟</span>
    
    
    
</div>

            

            <div class="post-body e-content">
                <h1 id="spark-streaming-kafka-原理"><a href="#spark-streaming-kafka-原理" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Spark Streaming Kafka 原理</h1>
<p>Kafka作为一个消息队列，具有很高的吞吐量，和Spark Streaming结合起来，可以实现高速实时的流处理。Spark Streaming在以前的版本支持两种方式读取Kafka，一种是通过 receiver 读取的方式，另一种是直接读取的方式。基于 receiver 方式的读取因为不太稳定，已经被最新版遗弃了，所以下面只讲直接读取的方式。</p>
<h2 id="工作流程"><a href="#工作流程" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>工作流程</h2>
<img src="spark-streaming-kafka-direct.svg">
<p>consumer 消费的 offset，这里会直接存储到 kafka里（__consumer_offset topic）。</p>
<ol>
<li>driver 首先会从 kafka 获取消费组的 offset，然后生成 KafkaRDD，KafkaRDD里的每个分区对应了 kafka 中一个 topic partition 的一小段数据。</li>
<li>executor 会根据这个信息，从 kafka 中读取对应的数据，注意到这里consumer使用的是 assign 分配模式，它不会去管理 offset。executor 读取到数据后，会进行相应的处理。</li>
<li>driver 收到所有 executor 的处理成功消息后，会向 kafka 提交 offset。</li>
</ol>
<p>下面可以看看它的示例代码</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="c1">// kafka 配置项，这里只列出一些重要项
</span><span class="c1"></span><span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;</span> <span class="n">kafkaParams</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&#34;bootstrap.servers&#34;</span><span class="o">,</span> <span class="s">&#34;localhost:9092,anotherhost:9092&#34;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&#34;group.id&#34;</span><span class="o">,</span> <span class="s">&#34;my_consumer_group&#34;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&#34;enable.auto.commit&#34;</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">topics</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&#34;topicA&#34;</span><span class="o">,</span> <span class="s">&#34;topicB&#34;</span><span class="o">);</span>

<span class="c1">// 创建 kafka stream
</span><span class="c1"></span><span class="n">JavaInputDStream</span><span class="o">&lt;</span><span class="n">ConsumerRecord</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">stream</span> <span class="o">=</span>
  <span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createDirectStream</span><span class="o">(</span>
    <span class="n">streamingContext</span><span class="o">,</span>
    <span class="n">LocationStrategies</span><span class="o">.</span><span class="na">PreferConsistent</span><span class="o">(),</span>
    <span class="n">ConsumerStrategies</span><span class="o">.&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span><span class="n">Subscribe</span><span class="o">(</span><span class="n">topics</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">)</span>
  <span class="o">);</span>

<span class="c1">// 调用 foreachRDD 来处理
</span><span class="c1"></span><span class="n">stream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">OffsetRange</span><span class="o">[]</span> <span class="n">offsetRanges</span> <span class="o">=</span> <span class="o">((</span><span class="n">HasOffsetRanges</span><span class="o">)</span> <span class="n">rdd</span><span class="o">.</span><span class="na">rdd</span><span class="o">()).</span><span class="na">offsetRanges</span><span class="o">();</span>  <span class="c1">// 在 driver执行
</span><span class="c1"></span>  <span class="n">rdd</span><span class="o">.</span><span class="na">mapPartitions</span><span class="o">(</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="o">{</span>
      <span class="c1">// .... 处理 
</span><span class="c1"></span>  <span class="o">})</span>
  <span class="c1">// 等待executor执行完任务，driver 端负责提交offset
</span><span class="c1"></span>  <span class="o">((</span><span class="n">CanCommitOffsets</span><span class="o">)</span> <span class="n">stream</span><span class="o">.</span><span class="na">inputDStream</span><span class="o">()).</span><span class="na">commitAsync</span><span class="o">(</span><span class="n">offsetRanges</span><span class="o">);</span>
<span class="o">});</span>
</code></pre></td></tr></table></div>
</div>
</div><h2 id="kafka数据流生成rdd"><a href="#kafka数据流生成rdd" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Kafka数据流生成RDD</h2>
<p>Kafka数据输入流由DirectKafkaInputDStream类表示，它会定期生成RDD。DirectKafkaInputDStream每次生成RDD的数据，都是读取kafka对应的topic所有分区，自从上次提交的offset一直到最新的offset到的数据。</p>
<p>DirectKafkaInputDStream继承InputDStream，它的compute方法负责生成RDD</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">DirectKafkaInputDStream</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span>
    <span class="nc">_ssc</span><span class="k">:</span> <span class="kt">StreamingContext</span><span class="o">,</span>
    <span class="n">locationStrategy</span><span class="k">:</span> <span class="kt">LocationStrategy</span><span class="o">,</span>
    <span class="n">consumerStrategy</span><span class="k">:</span> <span class="kt">ConsumerStrategy</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">],</span>
    <span class="n">ppc</span><span class="k">:</span> <span class="kt">PerPartitionConfig</span>
  <span class="o">)</span> <span class="k">extends</span> <span class="nc">InputDStream</span><span class="o">[</span><span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]](</span><span class="nc">_ssc</span><span class="o">)</span> <span class="k">with</span> <span class="nc">Logging</span> <span class="k">with</span> <span class="nc">CanCommitOffsets</span> <span class="o">{</span>

  <span class="c1">// 记录上次提交的offset， key为Kafka的topic分区，value为提交的offset
</span><span class="c1"></span>  <span class="k">protected</span> <span class="k">var</span> <span class="n">currentOffsets</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">TopicPartition</span>, <span class="kt">Long</span><span class="o">]()</span>
  
  <span class="k">override</span> <span class="k">def</span> <span class="n">compute</span><span class="o">(</span><span class="n">validTime</span><span class="k">:</span> <span class="kt">Time</span><span class="o">)</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">KafkaRDD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="c1">// 这里调用了latestOffsets方法，实时获取topic partition的最新offset
</span><span class="c1"></span>    <span class="c1">// clamp方法增加了限速的功能，计算出限速条件下，允许获取最大数据的offset
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">untilOffsets</span> <span class="k">=</span> <span class="n">clamp</span><span class="o">(</span><span class="n">latestOffsets</span><span class="o">())</span>
    <span class="c1">// 为每一个topic partition生成一个OffsetRange
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">offsetRanges</span> <span class="k">=</span> <span class="n">untilOffsets</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">tp</span><span class="o">,</span> <span class="n">uo</span><span class="o">)</span> <span class="k">=&gt;</span>
      <span class="c1">// 获取上次提交的offset
</span><span class="c1"></span>      <span class="k">val</span> <span class="n">fo</span> <span class="k">=</span> <span class="n">currentOffsets</span><span class="o">(</span><span class="n">tp</span><span class="o">)</span>
      <span class="c1">// 这批数据在kafka中的offset范围，是上次提交的offset到现在kafka中最新的offset
</span><span class="c1"></span>      <span class="nc">OffsetRange</span><span class="o">(</span><span class="n">tp</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">tp</span><span class="o">.</span><span class="n">partition</span><span class="o">,</span> <span class="n">fo</span><span class="o">,</span> <span class="n">uo</span><span class="o">)</span>
    <span class="o">}</span>
    
    <span class="k">val</span> <span class="n">useConsumerCache</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">getBoolean</span><span class="o">(</span><span class="s">&#34;spark.streaming.kafka.consumer.cache.enabled&#34;</span><span class="o">,</span>
      <span class="kc">true</span><span class="o">)</span>
    <span class="c1">// 生成KafkaRDD
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">rdd</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KafkaRDD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">context</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">,</span> <span class="n">executorKafkaParams</span><span class="o">,</span> <span class="n">offsetRanges</span><span class="o">.</span><span class="n">toArray</span><span class="o">,</span>
      <span class="n">getPreferredHosts</span><span class="o">,</span> <span class="n">useConsumerCache</span><span class="o">)</span>
    <span class="c1">// 更新currentOffsets
</span><span class="c1"></span>    <span class="n">currentOffsets</span> <span class="k">=</span> <span class="n">untilOffsets</span>
    <span class="c1">// 这里commitAll会去提交offset，但是DirectKafkaInputDStream是不管理offset，
</span><span class="c1"></span>    <span class="c1">// 只有用户主动调用了它的commitAsync方法，才会提交
</span><span class="c1"></span>    <span class="n">commitAll</span><span class="o">()</span>
    <span class="c1">// 返回 rdd
</span><span class="c1"></span>    <span class="nc">Some</span><span class="o">(</span><span class="n">rdd</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></td></tr></table></div>
</div>
</div><h2 id="kafkardd-原理"><a href="#kafkardd-原理" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>KafkaRDD 原理</h2>
<h3 id="kafkardd-分区原理"><a href="#kafkardd-分区原理" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>KafkaRDD 分区原理</h3>
<p>DirectKafkaInputDStream定期生成的RDD的类型是KafkaRDD。我们首先看看 KafkaRDD是如何划分分区的，它会根据从初始化时接收的offset信息参数，生成KafkaRDDPartition分区，每个分区对应着Kafka的一个topic partition 的一段数据。这段数据的信息由OffsetRange表示， 它保存了数据的位置。</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scala" data-lang="scala"><span class="k">final</span> <span class="k">class</span> <span class="nc">OffsetRange</span> <span class="k">private</span><span class="o">(</span>
    <span class="k">val</span> <span class="n">topic</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>   <span class="c1">// Kafka的topic名称
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">partition</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>  <span class="c1">// 该topic的partition
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">fromOffset</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span>  <span class="c1">// 起始offset
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">untilOffset</span><span class="k">:</span> <span class="kt">Long</span><span class="o">);</span>  <span class="c1">// 截至offset
</span></code></pre></td></tr></table></div>
</div>
</div><p>KafkaRDD继承RDD，它的getPartitions负责生成分区</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scala" data-lang="scala"><span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">class</span> <span class="nc">KafkaRDD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span>
    <span class="k">val</span> <span class="n">offsetRanges</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">OffsetRange</span><span class="o">]</span> 
<span class="o">)</span> <span class="k">extends</span> <span class="nc">RDD</span><span class="o">[</span><span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]](</span><span class="n">sc</span><span class="o">,</span> <span class="nc">Nil</span><span class="o">)</span> <span class="k">with</span> <span class="nc">Logging</span> <span class="k">with</span> <span class="nc">HasOffsetRanges</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="n">getPartitions</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Partition</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">offsetRanges</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">o</span><span class="o">,</span> <span class="n">i</span><span class="o">)</span> <span class="k">=&gt;</span>
        <span class="k">new</span> <span class="nc">KafkaRDDPartition</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">o</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">o</span><span class="o">.</span><span class="n">partition</span><span class="o">,</span> <span class="n">o</span><span class="o">.</span><span class="n">fromOffset</span><span class="o">,</span> <span class="n">o</span><span class="o">.</span><span class="n">untilOffset</span><span class="o">)</span>
    <span class="o">}.</span><span class="n">toArray</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></td></tr></table></div>
</div>
</div><p>接下来看看KafkaRDD的分区数据是如何读取的。它使用KafkaRDDIterator遍历数据，而KafkaRDDIterator的原理，是调用CachedKafkaConsumer的get方法获取数据</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">KafkaRDD</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">compute</span><span class="o">(</span><span class="n">thePart</span><span class="k">:</span> <span class="kt">Partition</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">TaskContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span> <span class="k">=</span>   <span class="o">{</span>
    <span class="c1">// 向下转型为KafkaRDDPartition
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">part</span> <span class="k">=</span> <span class="n">thePart</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">KafkaRDDPartition</span><span class="o">]</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">part</span><span class="o">.</span><span class="n">fromOffset</span> <span class="o">==</span> <span class="n">part</span><span class="o">.</span><span class="n">untilOffset</span><span class="o">)</span> <span class="o">{</span>
      <span class="nc">Iterator</span><span class="o">.</span><span class="n">empty</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="c1">// 返回迭代器
</span><span class="c1"></span>      <span class="k">new</span> <span class="nc">KafkaRDDIterator</span><span class="o">(</span><span class="n">part</span><span class="o">,</span> <span class="n">context</span><span class="o">)</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="k">class</span> <span class="nc">KafkaRDDIterator</span><span class="o">(</span>
      <span class="n">part</span><span class="k">:</span> <span class="kt">KafkaRDDPartition</span><span class="o">,</span>
      <span class="n">context</span><span class="k">:</span> <span class="kt">TaskContext</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Iterator</span><span class="o">[</span><span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]</span> <span class="o">{</span>

    <span class="n">logInfo</span><span class="o">(</span><span class="s">s&#34;Computing topic </span><span class="si">${</span><span class="n">part</span><span class="o">.</span><span class="n">topic</span><span class="si">}</span><span class="s">, partition </span><span class="si">${</span><span class="n">part</span><span class="o">.</span><span class="n">partition</span><span class="si">}</span><span class="s"> &#34;</span> <span class="o">+</span>
      <span class="s">s&#34;offsets </span><span class="si">${</span><span class="n">part</span><span class="o">.</span><span class="n">fromOffset</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">${</span><span class="n">part</span><span class="o">.</span><span class="n">untilOffset</span><span class="si">}</span><span class="s">&#34;</span><span class="o">)</span>

    <span class="c1">// 获取kafka消费者的groupId
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">groupId</span> <span class="k">=</span> <span class="n">kafkaParams</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="nc">GROUP_ID_CONFIG</span><span class="o">).</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span>
    <span class="c1">// 当整个spark streaming任务退出时，会调用closeIfNeeded方法关闭 kafka消费者
</span><span class="c1"></span>    <span class="n">context</span><span class="o">.</span><span class="n">addTaskCompletionListener</span><span class="o">{</span> <span class="n">context</span> <span class="k">=&gt;</span> <span class="n">closeIfNeeded</span><span class="o">()</span> <span class="o">}</span>
    <span class="c1">// 获取 kafka消费者
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">consumer</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="n">useConsumerCache</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// 如果支持 kafka消费者缓存，那么实例化CachedKafkaConsumer
</span><span class="c1"></span>      <span class="nc">CachedKafkaConsumer</span><span class="o">.</span><span class="n">init</span><span class="o">(</span><span class="n">cacheInitialCapacity</span><span class="o">,</span> <span class="n">cacheMaxCapacity</span><span class="o">,</span> <span class="n">cacheLoadFactor</span><span class="o">)</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="n">attemptNumber</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// 如果此次任务失败过，那么删除以前的 kafka 消费者
</span><span class="c1"></span>        <span class="nc">CachedKafkaConsumer</span><span class="o">.</span><span class="n">remove</span><span class="o">(</span><span class="n">groupId</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">partition</span><span class="o">)</span>
      <span class="o">}</span>
      <span class="c1">// 返回对应groupId，topic和partition的kafka消费者，如果有缓存则返回缓存的。
</span><span class="c1"></span>      <span class="nc">CachedKafkaConsumer</span><span class="o">.</span><span class="n">get</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">groupId</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">partition</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">)</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="c1">// 新建对应groupId，topic和partition的kafka消费者
</span><span class="c1"></span>      <span class="nc">CachedKafkaConsumer</span><span class="o">.</span><span class="n">getUncached</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">groupId</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">part</span><span class="o">.</span><span class="n">partition</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">// 关闭kafka消费者
</span><span class="c1"></span>    <span class="k">def</span> <span class="n">closeIfNeeded</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="k">if</span> <span class="o">(!</span><span class="n">useConsumerCache</span> <span class="o">&amp;&amp;</span> <span class="n">consumer</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">consumer</span><span class="o">.</span><span class="n">close</span>
      <span class="o">}</span>
    <span class="o">}</span>
    
    <span class="c1">// 只返回offset大于fromOffset，小于untilOffset的数据
</span><span class="c1"></span>    <span class="k">var</span> <span class="n">requestOffset</span> <span class="k">=</span> <span class="n">part</span><span class="o">.</span><span class="n">fromOffset</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">hasNext</span><span class="o">()</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="n">requestOffset</span> <span class="o">&lt;</span> <span class="n">part</span><span class="o">.</span><span class="n">untilOffset</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">next</span><span class="o">()</span><span class="k">:</span> <span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="n">assert</span><span class="o">(</span><span class="n">hasNext</span><span class="o">(),</span> <span class="s">&#34;Can&#39;t call getNext() once untilOffset has been reached&#34;</span><span class="o">)</span>
      <span class="c1">// 调用CachedKafkaConsumer的get方法返回一条数据
</span><span class="c1"></span>      <span class="k">val</span> <span class="n">r</span> <span class="k">=</span> <span class="n">consumer</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">requestOffset</span><span class="o">,</span> <span class="n">pollTimeout</span><span class="o">)</span>
      <span class="n">requestOffset</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">r</span>
    <span class="o">}</span>
  <span class="o">}</span>  
<span class="o">}</span>
</code></pre></td></tr></table></div>
</div>
</div><h3 id="cachedkafkaconsumer原理"><a href="#cachedkafkaconsumer原理" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>CachedKafkaConsumer原理</h3>
<p>CachedKafkaConsumer包含了KafkaConsumer实例，它是Kafka的消费者。CachedKafkaConsumer使用KafkaConsumer的assign模式，这种模式需要客户端自己管理offset。CachedKafkaConsumer是运行在 executor 节点上的，它只负责从kafka中读取指定的一段数据，所以不需要管理offset。</p>
<div class="highlight"><div class="chroma">
<div class="table-container"><table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">CachedKafkaConsumer</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="nc">private</span><span class="o">(</span>
  <span class="k">val</span> <span class="n">groupId</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
  <span class="k">val</span> <span class="n">topic</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
  <span class="k">val</span> <span class="n">partition</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>
  <span class="k">val</span> <span class="n">kafkaParams</span><span class="k">:</span> <span class="kt">ju.Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Object</span><span class="o">])</span> <span class="k">extends</span> <span class="nc">Logging</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">topicPartition</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TopicPartition</span><span class="o">(</span><span class="n">topic</span><span class="o">,</span> <span class="n">partition</span><span class="o">)</span>
  <span class="c1">// 实例化 KafkaConsumer，使用assign模式。这种模式需要自己维护offset
</span><span class="c1"></span>  <span class="k">protected</span> <span class="k">val</span> <span class="n">consumer</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">KafkaConsumer</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">](</span><span class="n">kafkaParams</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">tps</span> <span class="k">=</span> <span class="k">new</span> <span class="n">ju</span><span class="o">.</span><span class="nc">ArrayList</span><span class="o">[</span><span class="kt">TopicPartition</span><span class="o">]()</span>
    <span class="n">tps</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">topicPartition</span><span class="o">)</span>
    <span class="n">c</span><span class="o">.</span><span class="n">assign</span><span class="o">(</span><span class="n">tps</span><span class="o">)</span>
    <span class="n">c</span>
  <span class="o">}</span>

  <span class="c1">// 从kafka一次读取的数据是多条的，这里用buffer缓存读取的数据
</span><span class="c1"></span>  <span class="k">protected</span> <span class="k">var</span> <span class="n">buffer</span> <span class="k">=</span> <span class="n">ju</span><span class="o">.</span><span class="nc">Collections</span><span class="o">.</span><span class="n">emptyList</span><span class="o">[</span><span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]]().</span><span class="n">iterator</span>
  <span class="c1">// 下一条数据的offset
</span><span class="c1"></span>  <span class="k">protected</span> <span class="k">var</span> <span class="n">nextOffset</span> <span class="k">=</span> <span class="o">-</span><span class="mi">2L</span>

  <span class="k">def</span> <span class="n">close</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="n">consumer</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>

  <span class="c1">// 获取offset对应的数据
</span><span class="c1"></span>  <span class="k">def</span> <span class="n">get</span><span class="o">(</span><span class="n">offset</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">timeout</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">ConsumerRecord</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">logDebug</span><span class="o">(</span><span class="s">s&#34;Get </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> nextOffset </span><span class="si">$nextOffset</span><span class="s"> requested </span><span class="si">$offset</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="c1">// 如果要获取数据的offset不等于下一条数据的offset，则调用seek移动KafKaConsumer的位置
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(</span><span class="n">offset</span> <span class="o">!=</span> <span class="n">nextOffset</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">logInfo</span><span class="o">(</span><span class="s">s&#34;Initial fetch for </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> </span><span class="si">$offset</span><span class="s">&#34;</span><span class="o">)</span>
      <span class="c1">// 移动读取位置
</span><span class="c1"></span>      <span class="n">seek</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>
      <span class="c1">// 从kafka获取数据
</span><span class="c1"></span>      <span class="n">poll</span><span class="o">(</span><span class="n">timeout</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="c1">// 如果缓存的数据，已经读完，则调用poll从kafka中读取数据
</span><span class="c1"></span>    <span class="k">if</span> <span class="o">(!</span><span class="n">buffer</span><span class="o">.</span><span class="n">hasNext</span><span class="o">())</span> <span class="o">{</span> <span class="n">poll</span><span class="o">(</span><span class="n">timeout</span><span class="o">)</span> <span class="o">}</span>
    <span class="n">assert</span><span class="o">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">hasNext</span><span class="o">(),</span>
      <span class="s">s&#34;Failed to get records for </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> </span><span class="si">$offset</span><span class="s"> after polling for </span><span class="si">$timeout</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="c1">// 获取buffer的数据
</span><span class="c1"></span>    <span class="k">var</span> <span class="n">record</span> <span class="k">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>

    <span class="k">if</span> <span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="n">offset</span> <span class="o">!=</span> <span class="n">offset</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// 如果从buffer中获取的数据有问题，则需要重新从Kafka中读取数据
</span><span class="c1"></span>      <span class="n">logInfo</span><span class="o">(</span><span class="s">s&#34;Buffer miss for </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> </span><span class="si">$offset</span><span class="s">&#34;</span><span class="o">)</span>
      <span class="n">seek</span><span class="o">(</span><span class="n">offset</span><span class="o">)</span>
      <span class="n">poll</span><span class="o">(</span><span class="n">timeout</span><span class="o">)</span>
      <span class="n">assert</span><span class="o">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">hasNext</span><span class="o">(),</span>
        <span class="s">s&#34;Failed to get records for </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> </span><span class="si">$offset</span><span class="s"> after polling for </span><span class="si">$timeout</span><span class="s">&#34;</span><span class="o">)</span>
      <span class="c1">// 从buffer中读取数据
</span><span class="c1"></span>      <span class="n">record</span> <span class="k">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
      <span class="c1">// 检测该数据的offset是否等于预期
</span><span class="c1"></span>      <span class="n">assert</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="n">offset</span> <span class="o">==</span> <span class="n">offset</span><span class="o">,</span>
        <span class="s">s&#34;Got wrong record for </span><span class="si">$groupId</span><span class="s"> </span><span class="si">$topic</span><span class="s"> </span><span class="si">$partition</span><span class="s"> even after seeking to offset </span><span class="si">$offset</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="c1">// 更新nextOffset为当前offset+1
</span><span class="c1"></span>    <span class="n">nextOffset</span> <span class="k">=</span> <span class="n">offset</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">record</span>
  <span class="o">}</span>

  <span class="k">private</span> <span class="k">def</span> <span class="n">seek</span><span class="o">(</span><span class="n">offset</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// 调用KafKaConsumer的seek方法移动读取位置
</span><span class="c1"></span>    <span class="n">logDebug</span><span class="o">(</span><span class="s">s&#34;Seeking to </span><span class="si">$topicPartition</span><span class="s"> </span><span class="si">$offset</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="n">consumer</span><span class="o">.</span><span class="n">seek</span><span class="o">(</span><span class="n">topicPartition</span><span class="o">,</span> <span class="n">offset</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="k">private</span> <span class="k">def</span> <span class="n">poll</span><span class="o">(</span><span class="n">timeout</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="c1">// 调用poll方法读取数据
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">p</span> <span class="k">=</span> <span class="n">consumer</span><span class="o">.</span><span class="n">poll</span><span class="o">(</span><span class="n">timeout</span><span class="o">)</span>
    <span class="c1">// 获取该topic partition的数据
</span><span class="c1"></span>    <span class="k">val</span> <span class="n">r</span> <span class="k">=</span> <span class="n">p</span><span class="o">.</span><span class="n">records</span><span class="o">(</span><span class="n">topicPartition</span><span class="o">)</span>
    <span class="n">logDebug</span><span class="o">(</span><span class="s">s&#34;Polled </span><span class="si">${</span><span class="n">p</span><span class="o">.</span><span class="n">partitions</span><span class="o">()</span><span class="si">}</span><span class="s">  </span><span class="si">${</span><span class="n">r</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s">&#34;</span><span class="o">)</span>
    <span class="n">buffer</span> <span class="k">=</span> <span class="n">r</span><span class="o">.</span><span class="n">iterator</span>
  <span class="o">}</span>

<span class="o">}</span>
</code></pre></td></tr></table></div>
</div>
</div><p>从上面的代码可以看到，CachedKafkaConsumer会按照offset顺序的读取数据，并且offset还必须是连续的。如果Kafka开启了日志压缩功能，就会将相同key的数据压缩成一条，那么这样消息的offset就不会是连续的。这种情况下，spark streaming就会报错。上面的代码是spark 2.2版本的，后面的版本修复了这个问题，参见 <a href="https://github.com/apache/spark/pull/20572" target="_blank" rel="noopener">pull request</a>，增加了spark.streaming.kafka.allowNonConsecutiveOffsets配置，允许处理这种情况。</p>

            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text">作者</span>：<a href="https://zhmin.github.io/" class="p-author h-card" target="_blank" rel="noopener">zhmin</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text">链接</span>：<a href="/posts/spark-streaming-kafka/" target="_blank" rel="noopener">https://zhmin.github.io/posts/spark-streaming-kafka/</a></li>
            
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2021-10-22 14:44:34 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2021-10-22</text><text x="915" y="140" textLength="650" transform="scale(.1)">2021-10-22</text></g></svg>
        </span></div>



        


        


        
    
    
        <div class="related-posts">
            <h2 class="related-title">相关文章：<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/posts/spark-streaming-job-flow/" class="related-link">Spark Streaming 运行原理</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/spark-streaming-wal/" class="related-link">Spark Streaming WAL 原理</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/spark-streaming-receiver/" class="related-link">Spark Streaming 数据源读取</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/spark-external-shuffle-service/" class="related-link">Spark ExternalShuffleService 运行原理</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/spark-shuffle-reader/" class="related-link">Spark 读取 Shuffle 数据</a>
                    </li>
                
            </ul>
        </div>
    



        
    
        <div class="post-tags">
            
                
                
                
                
                    
                    <a href="/tags/spark/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>spark</a>
                
            
                
                
                
                
                    
                    <a href="/tags/streaming/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>streaming</a>
                
            
                
                
                
                
                    
                    <a href="/tags/kafka/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>kafka</a>
                
            
        </div>
    



        


        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/posts/spark-rpc-server/" rel="prev">&lt; Spark Rpc 服务端原理</a>
                </li>
            
            
                <li class="post-nav-next">
                    <a href="/posts/spark-streaming-job-flow/" rel="next">Spark Streaming 运行原理 &gt;</a>
                </li>
            
        </ul>
    



        
    

        

        

        
            <div id="vcomments"></div>
        

        

        
    



    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">©&nbsp;2019–2023&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;zhmin</div><div class="site-copyright"></div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="/rss.xml" target="_blank" rel="external noopener" title="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/zhmin" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li></ul>
    



            
        </div>
    </footer>


        </div>
        

        




    <script src="https://cdn.jsdelivr.net/npm/mermaid@8.8.3/dist/mermaid.min.js"></script>
<script>
    function magic(input) {
            input = input.replace(/&/g, '&amp;');
            input = input.replace(/</g, '&lt;');
            input = input.replace(/>/g, '&gt;');
            return input;
    }
    
    Array.from(document.getElementsByClassName("language-mermaid")).forEach(
        (el) => {
            content = magic(el.innerText)
            el.parentElement.outerHTML = `<div class="mermaid">${content}</div>`;
        }
    );
    let mermaidConfig = {
        startOnLoad: true,
        flowchart: {
            useMaxWidth: false,
            htmlLabels: true
        },
        theme: 'forest'
    };
    mermaid.initialize(mermaidConfig);
</script>



    

        

        
            <script>
    function loadComments() {
        if (typeof Valine === 'undefined') {
            var getScript = (options) => {
                var script = document.createElement('script');
                script.defer = true;
                script.crossOrigin = 'anonymous';
                Object.keys(options).forEach((key) => {
                    script[key] = options[key];
                });
                document.body.appendChild(script);
            };
            getScript({
                src: 'https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
                onload: () => {
                    newValine();
                }
            });
        } else {
            newValine();
        }
    }
    function newValine() {
        new Valine({
            el: '#vcomments',
            appId: 'rArgi8qro88zmWgSe68qAzgL-gzGzoHsz',
            appKey: 'R6YKDBN8HFTAqWcuSTzrSb6e',
            placeholder: 'Just go go',
            path: location.pathname,
            avatar: 'mm',
            meta: ["nick","mail","link"],
            pageSize:  10 ,
            lang: 'zh-cn',
            visitor:  true ,
            highlight:  true ,
            avatarForce:  false ,
            recordIP:  false ,
            serverURLs: '',
            emojiCDN: '',
            emojiMaps: {},
            enableQQ:  false ,
            requiredFields: []
        });
    }
</script>

        

        

        

    



    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    let imgNodes = document.querySelectorAll('div.post-body img');
    imgNodes = Array.from(imgNodes).filter(node => node.parentNode.tagName !== "A");

    mediumZoom(imgNodes, {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>







    </body>
</html>
